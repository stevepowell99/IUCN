# Brief overview of first results of scraping of web pages for small biodiversity projects in Central and West Africa. Work in Progress!

First, a short piece of code is written for each Fund website to download the data on small funds. In each case there are some compromises, problems and hacks. For example, in some cases the country is not specified but it can be guessed quite well by automatically searching for the names of the Central and West African countries in the title of each project.




```{r init,error=F,comment=NA, warning=F,cache=F, echo=F,message=FALSE}


library(knitr)
opts_chunk$set(error=F,comment=NA, warning=F,cache=T, echo=F,message=FALSE,fig.width=11,fig.height=7,fig.pos="h",results='asis')

library(ggplot2)
library(ggplot2)
library(WDI)
library(stringr)

library(RCurl)
library(XML)
clean=function(x)gsub("<[^>]*>", "",gsub("\n","",x))
xc=function(stri,sepp=" ") (strsplit(stri, sepp)[[1]]) 

html2txt <- function(str) {
      xpathApply(htmlParse(str, asText=TRUE),
                 "//body//text()", 
                 xmlValue)[[1]] 
}
```



```{r,echo=FALSE,cache=TRUE,message=FALSE,warning=FALSE,fig.height=9,fig.width=11}

usecache=T
if(usecache){
  mbz=readRDS(file = "objects/mbz")
  ppi=readRDS(file = "objects/ppi")
  monaco=readRDS(file = "objects/monaco")
  cepf=readRDS(file = "objects/cepf")
  whitley=readRDS(file = "objects/whitley")
  sos=readRDS(file = "objects/sos")
  mava=readRDS(file = "objects/mava")
  darwin=readRDS(file = "objects/darwin")
  gefs=readRDS(file = "objects/gefs")
  rufford=readRDS(file = "objects/rufford")

}


```

```{r}
# First, we grab the ISO country names from WB and regional codes (i.e. West and Central Africa) from UN and combine them, resulting in a vector of  codes for the countries we are interested in.
# 
# Then we grab some data using the API for the world bank WDIs (World Development Indicators,also available at google public data). 
# 
# Then we standardise them so that the mean is always zero and standard deviation 1, to make comparison between indicators easier.
# 
# The first chart shows time series for this first batch of indicators.


allcountries=read.csv("/home/steve/steve@promente.net/Methods/R-methods/mapAndCountryDataStuff/ISO-3166-Countries-with-Regional-Codes/all/all.csv", stringsAsFactors=F)

WAfCodes=allcountries[,2][allcountries[,7]=="11"]
CAfCodes=allcountries[,2][allcountries[,7]=="17"]
WAfNames=allcountries[,1][allcountries[,7]=="11"]
CAfNames=allcountries[,1][allcountries[,7]=="17"]

CWAfCodes=na.omit(c(WAfCodes,CAfCodes))
CWAfNames=na.omit(c(WAfNames,CAfNames))
# CWAfNames=na.omit(c("Democratic Republic of Congo","Cameroun","Congolaise",CWAfNames))
# if you do this, you have to extend the cwafcodes similarly, otherwise Rufford breaks.


if(!usecache){



inds=list()
inds$th=data.frame(t(WDIsearch("threatened")[1,]))
inds$bi=data.frame(t(WDIsearch("biodiversity")))
inds$ir=data.frame(t(WDIsearch("irrigated")[1,]))
inds$fo=data.frame(WDIsearch("forest")[c(3,8),])
inds$pr=data.frame(t(WDIsearch("Terrestrial")[2,]))
inds$ru=data.frame(WDIsearch("rural")[c(1,29,36),])
inds$hd=data.frame(t(WDIsearch("HDI")[3,]))
inds$fw=data.frame(t(WDIsearch("freshwater")[6,]))
inds$ag=data.frame(t(WDIsearch("agricultural")[4,]))
inds$ra=data.frame(t(WDIsearch("precipitation")))
inds$nr=data.frame((WDIsearch("natural resources")))

th=do.call(rbind,(inds))
# CWAfCodes=CWAfCodes[1:13]
all=lapply(1:nrow(th),function(x){
df=data.frame(WDI(country=CWAfCodes, indicator=th[x,1], start=2005, end=2014))
if(ncol(df)>0){
colnames(df)[3]="value"
df$indicator=th[x,2]
df$scaled=scale(df$value)
df
}
#   ddply(df,.variables = c("iso2c","country","indicator","year"),.fun = max)
})


alldf=(do.call(rbind,(all)))
alldf=na.omit(alldf)
saveRDS(alldf,"alldf")
} else alldf=readRDS(file = "objects/alldf")

label_wrap <- function(variable, value) {
  lapply(strwrap(as.character(value), width=12, simplify=FALSE), 
         paste, collapse="\n")
}  


```

```{r}
# ggplot(alldf[1:100,],aes(year,scaled))+facet_grid(country~indicator,scales = "free",labeller = label_wrap)+geom_line()
# +geom_rect(aes(x=1,y=0))+theme(
#   strip.text.y = element_text(angle=0),
#   strip.text.x = element_text(size=8),
#   axis.text.x=element_text(angle=270)
#   )

# ggplot(alldf[,],aes(year,scaled))+facet_grid(country~indicator,scales = "free",labeller = label_wrap)+
# geom_rect(aes(xmin=rep(min(year),nrow(alldf)),ymin=rep(min(scaled),nrow(alldf)),xmax=rep(max(year),nrow(alldf)),ymax=rep(max(scaled),nrow(alldf)),fill=scaled))+geom_line()+theme(
#   strip.text.y = element_text(angle=0),
#   strip.text.x = element_text(size=8),
#   axis.text.x=element_text(angle=270)
#   )
# alldf=alldf[1:100,]
# ggplot(alldf[,],aes(x=year,y=scaled,colour=scaled))+geom_line()

# 
# +facet_grid(country~indicator,scales = "free",labeller = label_wrap)+geom_line(size=2)+theme(
#   strip.text.y = element_text(angle=0),
#   strip.text.x = element_text(size=8),
#   axis.text.x=element_text(angle=270)
#   )

# The second graph is the beginning of an attempt to group the countries and indicators into meaningful clusters. Obviously we will need more indicators to make this useful. We extract only the latest data for each country for each indicator. 

# 
# TODO: 
# 
# - Note countries with any missing data are excluded here - that needs fixing.
# - Merge Cabo Verde and Cape Verde
```



```{r,echo=FALSE,cache=TRUE,message=FALSE,warning=FALSE,fig.height=9}
  library(dplyr)




# summarise(group_by(df,country,value,indicator),max(year))
alldf$scaled2=as.numeric(alldf$scaled)
alldf2=select(alldf,-scaled)
alldf.g <- group_by(alldf2, country,indicator)
latest=filter(alldf.g, year == max(year))
library(reshape)
library(pheatmap)
latest$value=latest$scaled2
c=as.matrix(cast(latest,country~indicator))
c=c[,-c(3)]
# library(impute)
# c=sapply(c,impute)
# pheatmap(na.omit(c),na.rm=T)

# ggplot(DF, aes(year, indicator, color=country))+geom_line(stat="identity")+xlab("Year")+ylab("")


# There isn't much grouping of indicators - the joins are quite high up in the dendrogram on the x axis. But the countries do cluster. 
```


```{r}
library(RColorBrewer)
data=(data.frame(xx=rownames(c),yy=c[,1]))
# worldmap(data)
# worldmap(data,mapTitle = "",breaks=-3:3,labels=xc("a b c d e f"))

```


```{r}
## Species maps using gbif
# key <- name_backbone(name='Puma concolor')$speciesKey
# dat <- occ_search(taxonKey=key, return='data', limit=300)
# gbifmap(input=dat)
# 
# gbifmap(input=dat,region="Burkina Faso")
# (burk<- isocodes[grep("Burkina", isocodes$name), "code"])
# dat <- occ_search(country=burk, return='data', limit=300)

```

```{r}
# just a bit of filtering for Bin Zayeed
# library(gnumeric)
# g=read.gnumeric.sheet(file = "/home/steve/steve@promente.net/Projects/IUCN/projectsAndFundsData/listOfGrants.ods", head = TRUE, sheet.name = "grants",stringsAsFactors=F) 
# 
# clist=c(CWAfNames,"Congo, Democratic Republic of (Congo-Kinshasa)","Côte d'Ivoire (Ivory Coast)","Saint Helena")
# 
# gg=read.gnumeric.sheet(file = "/home/steve/steve@promente.net/Projects/IUCN/projectsAndFundsData/listOfProjects.ods", head = TRUE, sheet.name = "leftOverFromMBZ",stringsAsFactors=F) 

```





```{r MBZ}
if(!usecache){

link="http://www.speciesconservation.org/case-studies-projects/index.php?order=GrantAcceptanceDateDESC&filter_amount_all_projects=&filter_iucn_all_projects=&filter_continent_all_projects=1&LocationCountry=&filter_species_all_projects=&GrantYear=&GrantMonth=&pagesize=999999&submit=Submit"

got=getURL(link)
# pargot=htmlTreeParse(got)
pargot=htmlParse(got)
xpargot=htmlTreeParse(got)
rows=xpathSApply(pargot,"//tr")
rows=xpathSApply(pargot,"//td")
rows2=sapply(rows,function(x)as(x,"character"))

new=list()
c=0
first=F
for(i in rows2){
  if(grepl("img ",i)){
    c=c+1
    new[[c]]=clean(i)
  } else new[[c]]=c(new[[c]],clean(i))
}
content=data.frame(t(do.call(cbind,new)),stringsAsFactors = F)[,-c(1,9:15)]
colnames(content)=xc("Title Continent Country SpeciesClass USD StartDate Details")
mbz=content

mbz$Fund="MBZ"
mbz=mbz[mbz$Continent=="Africa",]
mbz$USD=gsub("\\$","",mbz$USD)
mbz$USD=as.numeric(gsub(",","",mbz$USD))


# mbz$NGO=""
# mbz$EndDate=""
# mbz$Report=""
library(dplyr)
mbz=select(mbz,-Continent)
write.csv(mbz,"CSVs/MBZ.csv")
saveRDS(mbz,"objects/mbz")
} else mbz=readRDS(file = "objects/mbz")



# xrows=getNodeSet(pargot,"//tr")
# xxx=xrows[[2]]


```


```{r CEPF}

if(!usecache){

link="file:///media/steve/ssd/steve@promente.net/Projects/IUCN/projectsAndFundsData/CEPF.net%20-%20Search%20the%20Project%20Database.html"

got=getURL(link)


# kbbTree <- htmlTreeParse(got, asText = TRUE)
# kbbRoot <- xmlRoot(kbbTree)
# print(summary(kbbRoot))
# kbbdiv <- kbbRoot[["body"]][["div"]]

pargot=htmlTreeParse(got, useInternalNodes = TRUE)

root=xmlRoot(pargot)
NGO=xpathSApply(pargot,"//div[@class='filterData']//div[@class='grantee']",xmlValue)
StartDate=xpathSApply(pargot,"//div[@class='filterData']//div[@class='startDate']",xmlValue)
EndDate=xpathSApply(pargot,"//div[@class='filterData']//div[@class='endDate']",xmlValue)
USD=xpathSApply(pargot,"//div[@class='filterData']//div[@class='amount']",xmlValue)
Title=xpathSApply(pargot,"//div[@class='filterData']//div[@class='title']",xmlValue)
Details=xpathSApply(pargot,"//div[@class='filterData']//div[@class='description']",xmlValue)
# URL=xpathSApply(pargot,"//div[@class='filterData']//div[@class='documents']",xmlValue)
URL1=xpathSApply(pargot,"//div[@class='filterData']//div[@class='documents']")
URL=paste(sapply(URL1,function(x)xpathApply(x,"a//@href")[[1]]))

for(i in 1:length(URL)){
  system(paste0("wget -O pdfDownload/cepf",i,".pdf ",URL[i]))
}
for(i in 1:length(URL)){
  link=paste0("pdfDownload/cepf",i,".pdf")
  system(paste0("pdftotext ",link))
}
Report=list()
for(i in 1:length(URL)){
  link=paste0("pdfDownload/cepf",i,".txt")
  if(file.exists(link)) Report[[i]]=paste(readLines(link),collapse="\n") else Report[[i]]=""
}


# new=list()
# for(i in 1:length((rows))){
#   for(j in 1:length(names(rows[[i]]))) if(j==1) new[[i]]=rows[[i]][[j]] else new[[i]]=c(new[[i]],rows[[i]][[j]])
# }
# 
# s=sapply(new,function(x) sapply(x,function(y)clean(as(y,"character"))))
# cepf=data.frame(t(s))[,c(2,4,6,8,10,12,14)]
# colnames(cepf)=xc("NGO StartDate EndDate USD Title Details Report")
# cepf$Continent=""
# cepf$Country=""
# cepf$SpeciesClass=""
Report=paste(Report)
cepf=data.frame(NGO,StartDate,EndDate,USD,Title,Details,URL,Report)

cepf$Fund="CEPF"

write.csv(cepf,"CSVs/cepf.csv")
saveRDS(cepf,"objects/cepf")
} else cepf=readRDS(file = "objects/cepf")

```

```{r gef}
if(!usecache){

link="http://www.thegef.org/gef/project_list?keyword=&countryCode=&focalAreaCode=all&agencyCode=all&projectType=all&fundingSource=all&approvalFYFrom=all&approvalFYTo=all&ltgt=lt&ltgtAmt=&op=Search&form_build_id=form-9aCt9xrahrSa937JQ8KHkxySWx67BicZegxkUaiZDEY&form_id=prjsearch_searchfrm"
link="https://sgp.undp.org/index.php?option=com_sgpprojects&view=allprojects&limit=100&limitstart=100&paging=1"
got=getURL(link)

gef=read.csv("gef.csv",stringsAsFactors=F)
# colnames(gef)=xc("ID Country Title Focal.Area ")




#probably could use class=odd or even but I just downloaded the table as csv. but note none of the recipients seem to be small ngos. 
saveRDS(gef,"objects/gef")
} #else gef=readRDS(file = "objects/gef")




```


```{r ppi}
if(!usecache){

pp=xc("http://www.ffem.fr/accueil/PPI/recherche_Projets-PPI/ppi-afrique-centrale-orientale-australe/congo http://www.ffem.fr/accueil/PPI/recherche_Projets-PPI/ppi-afrique-centrale-orientale-australe/congo http://www.ffem.fr/accueil/PPI/recherche_Projets-PPI/ppi-afrique-centrale-orientale-australe/sao-tome-et-principe http://www.ffem.fr/accueil/PPI/recherche_Projets-PPI/ppi-afrique-centrale-orientale-australe/sao-tome-et-principe http://www.ffem.fr/accueil/PPI/recherche_Projets-PPI/ppi-afrique-centrale-orientale-australe/republique-centrafricaine http://www.ffem.fr/accueil/PPI/recherche_Projets-PPI/ppi-afrique-centrale-orientale-australe/tchad http://www.ffem.fr/accueil/PPI/recherche_Projets-PPI/ppi-afrique-ouest/benin http://www.ffem.fr/accueil/PPI/recherche_Projets-PPI/ppi-afrique-ouest/ghana http://www.ffem.fr/accueil/PPI/recherche_Projets-PPI/ppi-afrique-ouest/nigeria_1 http://www.ffem.fr/accueil/PPI/recherche_Projets-PPI/ppi-afrique-ouest/togo http://www.ffem.fr/accueil/PPI/recherche_Projets-PPI/ppi-afrique-ouest/burkina-faso http://www.ffem.fr/accueil/PPI/recherche_Projets-PPI/ppi-afrique-ouest/liberia http://www.ffem.fr/accueil/PPI/recherche_Projets-PPI/ppi-afrique-ouest/liberia http://www.ffem.fr/accueil/PPI/recherche_Projets-PPI/ppi-afrique-ouest/senegal")  

# linko=c("http://www.ffem.fr/accueil/PPI/recherche_Projets-PPI/ppi-afrique-centrale-orientale-australe/",
# pp=xc("cameroun congo republique-centrafricaine republique-democratique-du-congo")
# # pp=xc("republique-centrafricaine republique-democratique-du-congo")
content=list()
errors=list()

for(link in pp){
link2=link#paste0(linko,link,"/")
got=getURL(link2)
xpargot=htmlParse(got)
link3=xpathSApply(xpargot,"//div[@class='center_boxright']//a//@href")
for(link4 in link3){
# x=1
  got=getURL(paste0("http://www.ffem.fr",link4))
xpargot=htmlParse(got)
title=xpathSApply(xpargot,"//h1[@class='page_title']",xmlValue)
tmp=xpathSApply(xpargot,"//h2[@class='article_title']",xmlValue)
phase=tmp[[1]]

tmp2=xpathSApply(xpargot,"//div[@id='text_content']",xmlValue)
if(length(tmp2)>1) main=tmp2[[1]] else main=xpathSApply(xpargot,"//div[@class='article_item']//p",xmlValue)[[1]]
# 
# # if(length(main)<2)stop(link4)
# stop(as(main,"character"))
# browser()

implementer=str_match(main,"Porteur du projet : ([A-Z]*)")[2]
euroContribution=as.character(str_match(main,"(Contribution FFEM)(.)*([0-9]*[\\. ][0-9]*)")[1])
euroTotal=as.character(str_match(main,"([0-9]*[\\. ][0-9]* €)")[2])
# euroContribution=as.character(str_match(main,"([0-9]*\\.[0-9]* €)")[3])
# st=str_extract_all(main,"([0-9]*\\.[0-9]* €)")
# euroTotal=st[[1]]  
# if(length(st)>1) euroContribution=st[[2]]   else euroContribution=""
if(length(tmp2)>1) detail=tmp2[[2]] else detail=""
# browser()
URL=paste0("http://www.ffem.fr",link4)
if(length(main)>0)content[[link4]]=data.frame(Title=title,phase,More=main,Details=detail,NGO=implementer,euroContribution,euroTotal,URL) else errors[[link4]]=link4
}

}

ppi=data.frame(do.call(rbind.fill,content))
ppi$Fund="ppi"

ppi$Year=paste(sapply(paste(str_match(ppi$phase,"[1-4]")),function(x)switch(x, 
"1"={2008},
"2"={2010},
"3"={2012},
"4"={2014})
))


ppi$USD=as.numeric(gsub("( |\\.)","",str_match(ppi$euroContribution,"(([0-9])+(.| )?([0-9])+)")[,1]))*1.28

write.csv(ppi,"CSVs/ppi.csv")
saveRDS(ppi,"objects/ppi")
} else ppi=readRDS(file = "objects/ppi")

```

Another example: For the Petits Projects, neither the exact year of grant nor the start or end date are recorded, so the Year variable is calculated from the two-year Phases, which are given. 

```{r ppi2}
############## this is the big projects, stupid

if(F){
link="http://www.ffem.fr/cache/offonce/accueil/projets/recherche-de-projets?zone_filter=AO_FFEM&zone_filter=BF_FFEM&zone_filter=BJ_FFEM&zone_filter=CM_FFEM&zone_filter=CV_FFEM&zone_filter=CF_FFEM&zone_filter=CG_FFEM&zone_filter=CI_FFEM&zone_filter=GA_FFEM&zone_filter=GH_FFEM&zone_filter=GN_FFEM&zone_filter=LR_FFEM&zone_filter=ML_FFEM&zone_filter=NE_FFEM&zone_filter=NG_FFEM&zone_filter=CD_FFEM&zone_filter=ST_FFEM&zone_filter=SL_FFEM&zone_filter=SN_FFEM&zone_filter=TG_FFEM&grantyear_filter=all_categs4&grantyear_filter_all=all_categs4&grantyear_filter=1994&grantyear_filter=1995&grantyear_filter=1996&grantyear_filter=1997&grantyear_filter=1998&grantyear_filter=1999&grantyear_filter=2000&grantyear_filter=2001&grantyear_filter=2002&grantyear_filter=2003&grantyear_filter=2004&grantyear_filter=2005&grantyear_filter=2006&grantyear_filter=2007&grantyear_filter=2008&grantyear_filter=2009&grantyear_filter=2010&grantyear_filter=2011&grantyear_filter=2012&grantyear_filter=2013&grantyear_filter=2014"
got=getURL(link)
xpargot=htmlParse(got)
URL=xpathSApply(xpargot,"//a[@title='null']//@href")
Title=xpathSApply(xpargot,"//a[@title='null']",xmlValue)

content=list()
link=1
for(i in URL){
  link=paste0("http://www.ffem.fr",i)
  got=getURL(link)
xpargot=htmlParse(got)
rows=xpathSApply(xpargot,"//table[@id='projects']//tr//td")

}}

```



```{r darwin}
if(!usecache){

link="http://www.darwininitiative.org.uk/project/location/region/sub-saharan-africa/"
got=getURL(link)
xpargot=htmlParse(got)
rows=xpathSApply(xpargot,"//table[@id='projects']//tr//td")

content=list()
link=1
for(link in seq(from=1,by=5,to=length(rows))){
proj=paste0("proj",link)
  num=clean(as(rows[[link]],"character"))
  ref=clean(as(rows[[link+1]],"character"))
  det=clean(as(rows[[link+2]],"character"))
  dat=clean(as(rows[[link+3]],"character"))
  cou=clean(as(rows[[link+4]],"character"))
pounds=regmatches(det,regexpr("\\(([^)]+)0\\)",det))
# pounds2=regmatches(pounds,regexpr("([0123456789,\\.])?",pounds))
pounds2=gsub(")","",str_sub(pounds,12,999))

detail=gsub("\\(([^)]+)0\\)","",det)
web=paste0("http://www.darwininitiative.org.uk/project/",ref,"/")
# 
isCWA=sum(sapply(CWAfNames,function(x)grepl(x,cou)))>0
if(isCWA){
got=getURL(web)
xpargot=htmlParse(got)
more=xpathSApply(xpargot,"//div[@class='ncopyw']",xmlValue)

}else more=""

content[[proj]]=c(
  num,ref,dat,cou,pounds2,detail,web,more
  )
}
darwin=t(data.frame(content))
darwin=data.frame(darwin)
darwin$Fund="Darwin"
colnames(darwin)=xc("Ref ID StartDate Country GBP Title URL Details Fund")
darwin$USD=as.numeric(gsub(",","",darwin$GBP))*1.63
write.csv(darwin,"CSVs/darwin.csv")
saveRDS(darwin,"objects/darwin")
} else darwin=readRDS(file = "objects/darwin")


```


```{r monaco}
if(!usecache){

linko="http://www.fpa2.com/projets.php?categorie="
content=list()
for (lin in 1:8){
got=getURL(paste0(linko,lin))
xpargot=htmlParse(got)
rows=xpathSApply(xpargot,"//a[@class='link_actu rs_skip']//@href")


for(path in rows){
got=getURL(paste0("http://www.fpa2.com/",path))
xpargot=htmlParse(got)
title=xpathSApply(xpargot,"//h3",xmlValue)[[1]]
ngo=xpathSApply(xpargot,"//div[@class='row']//div[@style='margin:10px;']",xmlValue)[[1]]
all=xpathSApply(xpargot,"//div[@id='article_content']",xmlValue)
StartDate=xpathSApply(xpargot,"//div[@id='article_content']//strong",xmlValue)
if(length(StartDate)==0) StartDate=""
URL=paste0("http://www.fpa2.com/",path)

content[[path]]=data.frame(Title=title,NGO=ngo,Details=all,StartDate,URL)
content[[path]]$Fund="Monaco"

}
}
monaco=do.call(rbind.fill,content)
write.csv(monaco,"CSVs/monaco.csv")
saveRDS(monaco,"objects/monaco")
} else monaco=readRDS(file = "objects/monaco")


```



```{r whitley}
if(!usecache){

linko="http://whitleyaward.org/winners/"
content=list()
got=getURL(linko)
xpargot=htmlParse(got)

rows=xpathSApply(xpargot,"//tr[@class='winner-row']")
NGO=xpathSApply(xpargot,"//tr[@class='winner-row']//td[@class='po']",xmlValue)
Country=gsub("\t","",xpathSApply(xpargot,"//tr[@class='winner-row']//td[@class='pc']",xmlValue))
Type=xpathSApply(xpargot,"//tr[@class='winner-row']//td[@class='pt']",xmlValue)
StartDate=xpathSApply(xpargot,"//tr[@class='winner-row']//td[@class='py']",xmlValue)
URL=xpathSApply(xpargot,"//tr[@class='winner-row']//td[@class='po']//a//@href")


whitley=data.frame(NGO,Type,Country,StartDate,URL)
yes=sapply(whitley$Country,function(x){
  any(grepl(x,CWAfNames))
})

for(link in whitley$URL[yes]){
  got=getURL(link)
xpargot=htmlParse(got)
whitley$Title[whitley$URL==link]=xpathSApply(xpargot,"//div//*[contains(concat(' ', @class, ' '), ' standfirst ')]",xmlValue)
whitley$Details[whitley$URL==link]=xpathSApply(xpargot,"//div//*[contains(concat(' ', @class, ' '), ' hentry ')]",xmlValue)
}

whitley$StartDate=str_sub(whitley$StartDate,2,5)
whitley$Fund="whitley"
write.csv(whitley,"CSVs/whitley.csv")
saveRDS(whitley,"objects/whitley")
} else whitley=readRDS(file = "objects/whitley")


```



```{r mava}
if(!usecache){

linko="http://en.mava-foundation.org/what-we-fund/list-of-projects/coastal-west-africa/"
content=list()
got=getURL(linko)
xpargot=htmlParse(got)

main=xpathSApply(xpargot,"//*[contains(concat(' ', @class, ' '), ' postlist ')]")
Title=xpathSApply(xpargot,"//*[contains(concat(' ', @class, ' '), ' postlist ')]//p",xmlValue)
# URL=xpathSApply(xpargot,"//*[contains(concat(' ', @class, ' '), ' postlist ')]//a//@href")
rows=xpathSApply(xpargot,"//*[contains(concat(' ', @class, ' '), ' postlist ')]//td",xmlValue)
NGO=rows[1+(4*0:39)]
DurationMonths=rows[2+(4*0:39)]
EURrange=rows[3+(4*0:39)]
URL=rows[4+(4*0:39)]
USD
# 
# =switch(x, 
# "below 50’000"={
#   # case 'foo' here...
#   print('foo')
# },
# bar={
#   # case 'bar' here...
#   print('bar')    
# },
# {
#    print('default')
# }
# )


mava=data.frame(NGO,Title,URL,DurationMonths,EURrange)


mava$Fund="mava"
write.csv(mava,"CSVs/mava.csv")
saveRDS(mava,"objects/mava")
} else mava=readRDS(file = "objects/mava")


```


```{r sos}
if(!usecache){
# note I had to put a linebreak before some of the urls
# linko="http://www.sospecies.org/sos_projects/overview/"
rows=readLines("Save Our Species - Interactive Map.html")


Species=rows[1+(6*0:197)]
ID=rows[2+(6*0:197)]
Year=str_sub(ID,1,4)
URL=rows[3+(6*0:197)]
SpeciesClass=rows[4+(6*0:197)]
Country=rows[5+(6*0:197)]
Status=rows[6+(6*0:197)]


sos=data.frame(ID,URL,Year,Species,SpeciesClass,Country,Status)


sos$Fund="sos"
write.csv(sos,"CSVs/sos.csv")
saveRDS(sos,"objects/sos")
} else sos=readRDS(file = "objects/sos")


```



```{r gefs}
if(!usecache){
# linko="https://www.sgp.undp.org/index.php?option=com_sgpprojects&view=allprojects&limit=100&limitstart=200&paging=1&country=GHA"
# got=getURL(linko)
# xpargot=htmlParse(got)
## dos protection!!
linko="/media/steve/ssd/steve@promente.net/Projects/IUCN/projectsAndFundsData/gef.html"
rows=readLines(linko)
point=rows %in% xc("Ghana;Burkina faso;Niger",sep=";")
Country=rows[which(point)]
Type=rows[which(point)+2]
Year=rows[which(grepl("^20[01][0-9]$",rows))]
USD=as.numeric(gsub(",","",rows[which(grepl("^[0-9]*,[0-9]{3}\\.[0-9]{2}",rows))]))
ID=rows[which(grepl("^Project Number",rows))]
URLt=grepl("^</",rows)
URL=rows[which(URLt)]

a=cbind(which(grepl("^Project Number",rows)),which(grepl("^</",rows)))
Details=list()
for(i in 1:nrow(a)){
  Details[[i]]=paste(rows[a[i,][1]:a[i,][2]],collapse="\n")
}

gefs=data.frame(URL,Year,Type,Country,USD,Details=unlist(Details))

#dos protection!!
# pages=list()
# for(link in URL){
# linko=paste0("https://www.sgp.undp.org/",gsub("<|>","",link))
# xpargot=htmlParse(getURL(linko))
# pages[linko]=xpathSApply(xpargot,"//div[@class='maincontentarea_right']",xmlValue)
# pages[[link]]=xpargot
# }

# saveRDS(pages,"objects/gefspages")

gefs$Fund="gef-small"
write.csv(gefs,"CSVs/gefs.csv")
saveRDS(gefs,"objects/gefs")



} else gefs=readRDS(file = "objects/gefs")


```



```{r rufford}
if(!usecache){
  rm("rufford")
  j=1
linko=paste0("http://www.rufford.org/projects/byCountry/",CWAfCodes[j])
#   pagelist=c("","?page=1","?page=2","?page=3","?page=4","?page=5","?page=6")
while(j<length(CWAfCodes)-1){

# code=CWAfCodes[i]
#   
# warning(code)
if(url.exists(linko))got=getURL(linko)
xpargot=htmlParse(got)
NGO=xpathSApply(xpargot,"//div[@class='node-list']//h2[@class='title']",xmlValue)
if(is.null(NGO)){j=j+1;linko=paste0("http://www.rufford.org/projects/byCountry/",CWAfCodes[j]);next()}
if(length(NGO)==0){j=j+1;linko=paste0("http://www.rufford.org/projects/byCountry/",CWAfCodes[j]);next()}
URL=paste0("http://www.rufford.org",xpathSApply(xpargot,"//div[@class='node-list']//h2[@class='title']//a//@href"))
Title=xpathSApply(xpargot,"//div[@class='node-list']//h3",xmlValue)
tmp=xpathSApply(xpargot,"//div[@class='node-list']//div[@class='content']",xmlValue)
Details=sapply(1:length(tmp),function(x)gsub(Title[x],"",tmp[x]))

page=data.frame(NGO,URL,Title,Details)
page$Year=0
page$Report=""
page$More=""
page$Links=""
page$Country=CWAfNames[j]
page$link=linko

for(q in 1:length(URL)){
xpargot2=htmlParse(getURL(URL[q]))
tab=xpathSApply(xpargot2,"//table[@class='project_info_table']//td",xmlValue)
page$Year[q]=str_match(tab[4],"2[0-1][0-9][0-9]")
page$Report[q]=paste0(xpathSApply(xpargot2,"//div[@class='content']//p",xmlValue),collapse="\n")
page$Links[q]=paste0(xpathSApply(xpargot2,"//div[@class='relativity_child']//a//@href"),collapse=",")
}

if(exists("rufford"))rufford=do.call(rbind,list(page,rufford))else rufford=page
saveRDS(file="rufford",object=rufford)


xx=xpathSApply(xpargot,"//li[@class='pager-next']//a//@href")
if(is.null(xx)) {
  j=j+1;linko=paste0("http://www.rufford.org/projects/byCountry/",CWAfCodes[j]);cat(j)
                 } else {linko=paste0("http://www.rufford.org",xx);cat("paging ");cat(xx)}
cat(linko)
}
rufford$Fund="Rufford"
rufford$Type="Individual"

write.csv(rufford,"CSVs/rufford.csv")
saveRDS(rufford,"objects/rufford")

} else rufford=readRDS(file = "objects/rufford")


```

This results in one database of projects for each Fund. 


These databases are then merged automatically into one. Some standard variables are available for all or nearly all of the Funds, whereas other variables (like "status") are often not given and so are not generally available. These additional variables are not considered in the preliminary analyses below.

A slightly earlier version of the resulting database can be viewed: [here](https://www.google.com/fusiontables/DataSource?docid=1AP3Mn61QvW6RHDARhZvHp9FfubMgjSjJh6J18PZl).

# About this approach

- First, isn't this a *stupid thing to do*? Wouldn't it be better to write to each funding body and ask them for the official datasets? - well, perhaps. I did actually write to them all and have a very positive response from GEF and a partial response from MBZ; still waiting for some others. 
    - Advantages of this approach:
        - Of course, even if we were to get a clean dataset from each funding body, they might not include all the data we want. 
        - It updates the dataset as the websites are updated without mailing stuff about. 
    - Disadvantages
        - If the funders did mail us their data, they might include more data than is available at the websites. 
        - "Scraping" might look aggressive or impolite. 
- Second, this is work in progress; 
    - some of the data which is in fact available at the sites have either not yet been scraped or has not yet been processed for this analysis.
    - there are other funds with project websites which I have not yet accessed.
    - it would be relatively trivial to extend this work to cover the whole world (in fact in some ways it would be easier because I wouldn't have to filter by country; on the other hand the computer would take a bit longer and get a bit hotter.)
    


```{r joining}
library(plyr)

all=rbind.fill(ppi,cepf,mbz,monaco,darwin,whitley,mava,sos,gefs,rufford)
all$Country=as.character(all$Country)
all$USD=as.numeric(all$USD)
target=with(all,paste(Country,Details,More,Title,NGO))

all$CWA=sapply(target,function(x)sum(sapply(CWAfNames,function(y){
grepl(y,x)
}))>0
)

all$CentralWestAfrica=ifelse(all$CWA,"CWAfrica","Elsewhere")

all$CountryGuess=str_match(target,paste0(CWAfNames,collapse='|'))
all$Country=ifelse(is.na(all$Country),all$CountryGuess,all$Country)
# all=all[all$CentralWestAfrica=="CWAfrica",]

# all$Primates=sapply(target,function(x)sum(sapply(CWAfNames,function(y){
# grepl(y,x)
# }))>0
# )
all$Year=as.numeric(ifelse(is.na(all$Year),str_match(all$StartDate,"2[01][0-9]{2}"),all$Year))
library(dplyr)
# all2=select(all,-c(euroContribution,euroTotal,CountryGuess,GBP,ID,EURrange))
all2=select(all,c(Fund,Country,Title,USD,Year,NGO,Details,Species,URL))
write.csv(all2,"CSVs/all.csv")
saveRDS(all,"all")

```

# Overview of the scraped data



```{r tabs,results='asis',comment="",cache=FALSE}
library(dplyr)
# table(all$Fund,all$Year)
# table(all$Fund,all$CentralWestAfrica)
s=select(all,Fund,USD,Year,Country,CWA,URL,Report,NGO)
k=summarise(group_by(s,Fund),Number_projects=length(Fund),Num_in_CWAfrica=sum(CWA),max_USD=max(USD,na.rm=T),min_USD=min(USD,na.rm=T),First=min(Year,na.rm=T),Last=max(Year,na.rm=T),Num_countries=length(unique(Country)),Has_URLs=all(!is.na(URL)),Has_Reports=all(!is.na(Report)),Has_Recipient_Namess=all(!is.na(NGO)))

# saveRDS(k,"k") #I havew no idea why this is necessary but something wants to collapse the summarised table to one line

k=readRDS("k")
kable(k)


```

There are several things to note.

- There are so many GEF small-projects that so far only the first 100 projects from three countries have been included. Downloading from the GEF website is also tiresome because the site is DOS-protected.
- In some cases, projects from outside Central & West Africa, and projects over 50K USD are also included. This is quite useful for the purposes of comparison but is not very systematic. 
- The column "has URLs" says whether separate web pages are also given for each project. In some cases the script also scrapes and includes the contents of these pages, in some cases not.
- The column "has Reports" says whether the full-text of the project reports is also included in the database. So for example for the CEPF website, the text of the attached project PDFs is automatically added to the database. In principle it would be possible to also scan this text to extract key data but this depends on how systematically the reports have been written. (In the Fusion Tables version of the resulting [database](https://www.google.com/fusiontables/DataSource?docid=1AP3Mn61QvW6RHDARhZvHp9FfubMgjSjJh6J18PZl), the Reports column is hidden because there is so much text.)
- It is easy enough to provide country-level maps based on this data (e.g. average grant per country) but I have not attempted to scrape sub-country locations; this would be possible though time-consuming using Google Maps APIs.


# Some first analyses

## Grant size over time

```{r scales}
library(scales)

# ggplot(all[all$CentralWestAfrica,],aes(USD,group=Fund,colour=Fund))+geom_density()+scale_x_log10()+facet_wrap(~Year)

ggplot(all[],aes(Year,USD,group=Fund,colour=Fund))+geom_jitter(alpha=.5,size=5)+scale_y_log10()+geom_hline(yintercept=c(5000,50000,500000), linetype="dotted")+ylab("USD logarithmic")#+theme(axis.text.x=element_text(angle=90))

```

## Number of projects per year

```{r}

ggplot(all,aes(Year,fill=Fund))+geom_histogram()+ scale_fill_discrete(guide = guide_legend(reverse=TRUE))+ylab("Number of projects")
```

This apparent growth is mostly due to the MBZ (Bin Zajed) projects.

```{r maps}

# library(choroplethr)
# df=data.frame("region"=xc("Benin Togo Ghana Benin Togo Ghana"),value=6:11,thing=xc("1 2 1 2 1 2"))
# choroplethr(df,"world")+coord_cartesian(xlim = c(-30,30),ylim = c(-30,30))+geom_point(x=1,y=1,size=9)+facet_grid(.~thing)
# 
# 
# library(maptools)
# library(ggplot2)
# library(ggmap)
#  
# # read administrative boundaries (change folder appropriately)
# eurMap <- readShapePoly(fn="NUTS_2010_60M_SH/Shape/data/NUTS_RG_60M_2010")
#  
# # read downloaded data (change folder appropriately)
# eurEdu <- read.csv("educ_thexp_1_Data.csv", stringsAsFactors = F)
# eurEdu$Value <- as.double(eurEdu$Value) #format as numeric
#  
# # merge map and data
# eurEduMapDf <- merge(eurMapDf, eurEdu, by.x="id", by.y="GEO")
# eurEduMapDf <- eurEduMapDf[order(eurEduMapDf$order),]
#  
# #limit data to main Europe
# europe.limits <- geocode(c("Cape Fligely, Rudolf Island, Franz Josef Land, Russia", "Gavdos, Greece", "Faja Grande, Azores", "Severny Island, Novaya Zemlya, Russia"))
#  
# eurEduMapDf <- subset(eurEduMapDf, long > min(europe.limits$lon) & long < max(europe.limits$lon) & lat > min(europe.limits$lat) & lat < max(europe.limits$lat))
#  
# # ggplot mapping
# # data layer
# m0 <- ggplot(data=eurEduMapDf)
# # empty map (only borders)
# m1 <- m0 + geom_path(aes(x=long, y=lat, group=group), color='gray') + coord_equal()
#  
# # fill with education expenditure data
# m2 <- m1 + geom_polygon(aes(x=long, y=lat, group=group, fill=Value))
#  
# # inverse order (to have visible borders)
# m0 <- ggplot(data=eurEduMapDf)
# m1 <- m0 + geom_polygon(aes(x=long, y=lat, group=group, fill=Value)) + coord_equal()
# m2 <- m1 + geom_path(aes(x=long, y=lat, group=group), color='black')
# m2
#  
# # over a GoogleMap (not working if not correctly projected)
# map <- get_map(location = 'Europe', zoom=4)
# m0 <- ggmap(map)
# m1 <- m0 + geom_polygon(aes(x=long, y=lat, group=group, fill=Value), data=eurEduMapDf, alpha=.9)
# m2 <- m1 + geom_path(aes(x=long, y=lat, group=group), data=eurEduMapDf, color='black')
#  
# # add text
# library(doBy)
# txtVal <- summaryBy(long + lat + Value ~ id, data=eurEduMapDf, FUN=mean, keep.names=T)
# m3 <- m2 + geom_text(aes(x=long, y=lat, label=Value), data=txtVal, col="yellow", cex=3)
```


## Do funds give grants to the same recipient more than once?

```{r all-or-some,results='asis'}
aa=as.matrix(table(all$NGO,all$Fund))
aaa=data.frame(aa)
colnames(aaa)=xc("x Fund Frequency")
aaa$Frequency[aaa$Frequency==0]=NA
aaa=na.omit(aaa)
ggplot(aaa,aes(Frequency,fill=Fund))+geom_bar()+xlab("Number of grants to same recipient, per Fund")

# kable(table(aaa$Frequency,aaa$Fund))
```

It is quite rare that a grant is given to the same recipient more than once. Monaco have 9 recipients with a second grant, and Rufford goes up to three. Only CPEF consistently funds recipients more than two or three times, in fact more than half of their grants go to NGOs with more than one grant. 

Of course, the recipient name might have changed slightly or be spelt differently, which would result in underestimates of the number receiving more than one grant.

## Average value of small grants by country

```{r}
library(choroplethr)

all3=filter(all,CountryGuess!="Côte d'Ivoire")
# all3=filter(all4,CountryGuess!="Cape Verde")
xx=select(all3,region=CountryGuess,value=USD,CWA) %>% filter(CWA) %>% group_by(region) %>% summarise(value=mean(value,na.rm=T))
# saveRDS(xx,"xx")
xx=readRDS("xx")
# df=data.frame("region"=xc("Benin Togo Ghana Benin Togo Ghana"),value=6:11,thing=xc("1 2 1 2 1 2"))
choroplethr(xx,"world",num_buckets = 5,warn_na = F)+coord_cartesian(xlim = c(-30,30),ylim = c(-30,30))+geom_point(x=1,y=1,size=9)+scale_fill_brewer(na.value="white",name="Average USD")#+ggtitle("Map of average value of small grants")

```

## Total value of small grants by country

```{r}
library(choroplethr)

all3=filter(all,CountryGuess!="Côte d'Ivoire")
# all3=filter(all4,CountryGuess!="Cape Verde")
xx=select(all3,region=CountryGuess,value=USD,CWA) %>% filter(CWA) %>% group_by(region) %>% summarise(value=sum(value,na.rm=T))
# saveRDS(xx,"xx")
xx=readRDS("xx")
# df=data.frame("region"=xc("Benin Togo Ghana Benin Togo Ghana"),value=6:11,thing=xc("1 2 1 2 1 2"))
choroplethr(xx,"world",num_buckets = 5,warn_na = F)+coord_cartesian(xlim = c(-30,30),ylim = c(-30,30))+geom_point(x=1,y=1,size=9)+scale_fill_brewer(na.value="white",name="Total USD")#+ggtitle("Map of average value of small grants")

```

```{r}
link="http://iatiregistry.org/api/3/action/package_search?q=extras_filetype:activity%20AND%20extras_country:gh"
got=getURL(link)
xpargot=htmlParse(got)

```


-----

## Reproducibility information

This document was produced from an Rmarkdown script in the R program ([R](http://r-project.org)) using RCurl and other packages.
```{r}
# sessionInfo()
```

